# v5.12.0 - Vanta Compliance CloudWatch Alarms

**Status:** PLANNED - Ready for implementation
**Target Release:** v5.12.0 (Next minor release)
**Type:** Feature addition (non-breaking)
**Purpose:** Enable Vanta compliance with optional CloudWatch alarms for ALB monitoring

---

## Overview

Add optional CloudWatch alarms for Application Load Balancer monitoring to support Vanta compliance requirements.
All alarms are **optional in v5.12.0** but will become **required in v6.0.0**.

**Key Principles:**
- Non-breaking change: All new variables have sensible defaults
- Opt-in: Users must explicitly configure `alarm_emails` to enable alarms
- Clear warning: Check block warns users if alarms are not configured
- Forward-compatible: Prepares users for v6.0.0 requirement

---

## Implementation Progress

### Phase 1: Planning
- [x] Create implementation plan document
- [x] Update v6.0.0 breaking changes plan
- [x] Define variables and their defaults
- [x] Design alarm thresholds and metrics
- [x] Review plan with team (if applicable)

### Phase 2: Infrastructure Code
- [x] Create `alarms.tf` with SNS topic resource
- [x] Add SNS email subscriptions to `alarms.tf`
- [x] Add CloudWatch alarm for unhealthy host count
- [x] Add CloudWatch alarm for target response time (latency)
- [x] Add CloudWatch alarm for low success rate (5xx errors)
- [x] Add CloudWatch alarm for high CPU utilization
- [x] Add alarm variables to `variables.tf`
- [x] Add alarm locals to `locals.tf`
- [x] Add alarm outputs to `outputs.tf`
- [x] Add warning check block to `deprecations.tf`
- [x] Remove legacy CPU alarm from `cloudwatch.tf`

### Phase 3: Testing
- [x] Extend `tests/test_create_lb.py` with alarm verification
- [x] Test: SNS topic created with correct configuration
- [x] Test: Email subscription exists (PendingConfirmation OK)
- [x] Test: All 4 CloudWatch alarms created with correct thresholds
- [x] Test: Alarm dimensions reference correct resources
- [ ] Manual test: Deploy with real email and confirm subscription
- [ ] Manual test: Trigger unhealthy host alarm
- [ ] Manual test: Trigger CPU alarm with stress test
- [ ] Manual test: Verify email notifications received

### Phase 4: Documentation
- [ ] Run terraform-docs to update README.md
- [ ] Update CHANGELOG.md with new features
- [ ] Verify variable descriptions are clear
- [ ] Verify examples in plan are accurate

### Phase 5: Review and Release
- [ ] Code review (self or team)
- [ ] All tests passing
- [ ] Pre-commit hooks passing
- [ ] Commit changes with conventional commit message
- [ ] Create pull request
- [ ] Merge to main
- [ ] Tag and release v5.12.0

---

## Vanta Requirements

Vanta requires monitoring for the following metrics:

**ALB Metrics:**
1. **Load balancer unhealthy host count monitored (AWS)**
2. **Load balancer latency monitored**
3. **Load balancer server errors monitored (AWS)**

**EC2/ASG Metrics:**
4. **Server CPU monitored (AWS)**

---

## Implementation Design

### 1. New Variables

```hcl
variable "alarm_emails" {
  description = <<-EOF
    List of email addresses to receive CloudWatch alarm notifications for ALB monitoring.
    AWS will send confirmation emails that must be accepted.

    **Vanta Compliance Requirements:**
    When configured, creates CloudWatch alarms for:
    - Load balancer unhealthy host count monitoring
    - Load balancer latency monitoring
    - Load balancer server errors (5xx) monitoring

    **Example:**
    ```
    alarm_emails = ["ops-team@example.com", "on-call@example.com"]
    ```

    ⚠️  **FUTURE REQUIREMENT:** In v6.0.0, at least one email address will be required.
    See UPGRADE-6.0.md for migration details.
  EOF
  type        = list(string)
  default     = []
}

variable "alarm_topic_arns" {
  description = <<-EOF
    List of existing SNS topic ARNs to send ALB alarms to.
    Use this for advanced integrations like PagerDuty, Slack, OpsGenie, etc.

    These topics will receive notifications in addition to any configured alarm_emails.

    **Example:**
    ```
    alarm_topic_arns = [
      "arn:aws:sns:us-east-1:123456789012:pagerduty-critical",
      "arn:aws:sns:us-east-1:123456789012:slack-alerts"
    ]
    ```
  EOF
  type        = list(string)
  default     = []
}

variable "alarm_unhealthy_host_threshold" {
  description = <<-EOF
    Number of unhealthy hosts that triggers an alarm.

    Recommended: Set to 1 for production environments to catch issues immediately.
    For development/staging, you might set higher or disable alarms entirely.
  EOF
  type        = number
  default     = 1
}

variable "alarm_target_response_time_threshold" {
  description = <<-EOF
    Target response time threshold in seconds that triggers a latency alarm.

    If not specified, defaults to 80% of alb_idle_timeout to alert before
    connections start timing out.

    Example: With default alb_idle_timeout=60s, this will default to 48s.

    You can override this for more aggressive monitoring:
    - API services: 0.5 - 1.0 seconds
    - Web applications: 1.0 - 2.0 seconds
    - Backend services: 2.0 - 5.0 seconds
  EOF
  type        = number
  default     = null
}

variable "alarm_success_rate_threshold" {
  description = <<-EOF
    Minimum success rate (percentage) before triggering an alarm.

    Success rate = (non-5xx responses) / (total responses) * 100

    This is smarter than a raw error count because it scales with traffic volume.
    A 1% error rate means the same thing whether you have 100 or 100,000 requests.

    **Default:** 99.0 (alerts when error rate exceeds 1%)

    **Examples:**
    - 99.9 = Alert when more than 0.1% of requests fail (very strict SLO)
    - 99.0 = Alert when more than 1% of requests fail (recommended)
    - 95.0 = Alert when more than 5% of requests fail (lenient)

    **Note:** Alarms won't trigger during periods with zero traffic.
  EOF
  type        = number
  default     = 99.0

  validation {
    condition     = var.alarm_success_rate_threshold >= 0 && var.alarm_success_rate_threshold <= 100
    error_message = "Success rate threshold must be between 0 and 100 (percentage)"
  }
}

variable "alarm_success_rate_period" {
  description = <<-EOF
    Time period (in seconds) over which to calculate the success rate.

    Longer periods provide more statistical stability, especially important
    for low-traffic sites where individual errors can skew short-term rates.

    **Default:** 300 seconds (5 minutes)

    **Recommendations by traffic volume:**
    - Very low traffic (< 1 req/min):   3600s (1 hour) for statistical significance
    - Low traffic (1-10 req/min):       900s (15 min)
    - Medium traffic (10-100 req/min):  300s (5 min) - default
    - High traffic (> 100 req/min):     60s (1 min) for faster detection

    **Detection time:** With evaluation_periods=2:
    - 3600s (1 hour) = 2 hour detection time
    - 900s (15 min) = 30 minute detection time
    - 300s (5 min) = 10 minute detection time
    - 60s (1 min) = 2 minute detection time

    **Example for low-traffic site:**
    ```
    alarm_success_rate_period = 3600  # 1 hour window
    alarm_success_rate_threshold = 99.0
    ```
    With 10 requests/hour, allows 1 error before alarming.
  EOF
  type        = number
  default     = 300  # 5 minutes

  validation {
    condition     = contains([60, 300, 900, 3600], var.alarm_success_rate_period)
    error_message = "Period must be 60 (1 min), 300 (5 min), 900 (15 min), or 3600 (1 hour)"
  }
}

variable "alarm_cpu_utilization_threshold" {
  description = <<-EOF
    CPU utilization percentage that triggers an alarm.

    This alarm detects when autoscaling FAILS to keep up with demand, which may indicate:
    - ASG reached max_size (cannot scale further)
    - New instances failing to provision
    - New instances not becoming healthy
    - Infrastructure capacity/quota issues

    **Automatic calculation:**
    If not specified, defaults to autoscaling_target_cpu_load + 30%.
    This provides a buffer for autoscaling to respond before alarming.

    **Example automatic thresholds:**
    - autoscaling_target_cpu_load = 60%: alarm at 90%
    - autoscaling_target_cpu_load = 70%: alarm at 100%

    **How it works:**
    When CPU exceeds target (60% default), ASG launches new instances (~5-10 min).
    If CPU stays high for 10 minutes (period × evaluation_periods), autoscaling has failed - time to alert!

    **Override for custom thresholds:**
    ```
    alarm_cpu_utilization_threshold = 85  # Explicit threshold
    ```

    **Note:** This is a Vanta compliance requirement (Server CPU monitored).
  EOF
  type        = number
  default     = null

  validation {
    condition     = var.alarm_cpu_utilization_threshold == null || (var.alarm_cpu_utilization_threshold > 0 && var.alarm_cpu_utilization_threshold <= 100)
    error_message = "CPU utilization threshold must be between 0 and 100 (percentage)"
  }
}

variable "alarm_evaluation_periods" {
  description = <<-EOF
    Number of periods over which to compare the metric to the threshold.

    With 1-minute periods, setting this to 2 means the alarm must breach
    for 2 consecutive minutes before triggering.
  EOF
  type        = number
  default     = 2

  validation {
    condition     = var.alarm_evaluation_periods >= 1
    error_message = "Evaluation periods must be at least 1"
  }
}
```

**Rationale for thresholds:**
- Configurable thresholds allow users to tune alarms for their specific workload
- Sensible defaults based on common production requirements
- Vanta focuses on "monitored" - thresholds are implementation details

---

### 2. New Locals

```hcl
locals {
  # Determine if alarms should be created
  alarms_enabled = length(var.alarm_emails) > 0 || length(var.alarm_topic_arns) > 0

  # Calculate response time threshold: default to 80% of idle timeout
  alarm_target_response_time = coalesce(
    var.alarm_target_response_time_threshold,
    var.alb_idle_timeout * 0.8
  )

  # Calculate CPU threshold: default to autoscaling target + 30%
  # Cap at 100% to avoid invalid thresholds
  alarm_cpu_threshold = min(
    coalesce(
      var.alarm_cpu_utilization_threshold,
      var.autoscaling_target_cpu_load + 30
    ),
    100
  )

  # SNS topic ARNs to send alarms to
  alarm_sns_topics = concat(
    length(var.alarm_emails) > 0 ? [aws_sns_topic.alarms[0].arn] : [],
    var.alarm_topic_arns
  )
}
```

---

### 3. New SNS Topic and Subscriptions

**File:** `alarms.tf` (new file)

```hcl
# SNS Topic for alarm notifications
resource "aws_sns_topic" "alarms" {
  count = length(var.alarm_emails) > 0 ? 1 : 0

  name              = "${var.alb_name_prefix}-alb-alarms"
  display_name      = "ALB Alarms for ${var.alb_name_prefix}"
  kms_master_key_id = "alias/aws/sns"  # Encrypt SNS topic

  tags = merge(
    local.common_tags,
    {
      Name        = "${var.alb_name_prefix}-alb-alarms"
      Description = "CloudWatch alarms for ALB monitoring (Vanta compliance)"
    }
  )
}

# Email subscriptions for the alarm topic
resource "aws_sns_topic_subscription" "alarm_emails" {
  count = length(var.alarm_emails)

  topic_arn = aws_sns_topic.alarms[0].arn
  protocol  = "email"
  endpoint  = var.alarm_emails[count.index]
}
```

**Rationale:**
- Creates SNS topic only if `alarm_emails` is provided
- Email subscriptions require manual confirmation (AWS security)
- Topic is encrypted for security
- Proper tagging for identification

---

### 4. CloudWatch Alarms

**File:** `alarms.tf` (continued)

#### Alarm 1: Unhealthy Host Count

```hcl
resource "aws_cloudwatch_metric_alarm" "unhealthy_host_count" {
  count = local.alarms_enabled ? 1 : 0

  alarm_name          = "${var.alb_name_prefix}-unhealthy-hosts"
  alarm_description   = "Triggers when unhealthy host count exceeds ${var.alarm_unhealthy_host_threshold} (Vanta compliance)"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = var.alarm_evaluation_periods
  metric_name         = "UnHealthyHostCount"
  namespace           = "AWS/ApplicationELB"
  period              = 60  # 1 minute
  statistic           = "Average"
  threshold           = var.alarm_unhealthy_host_threshold
  treat_missing_data  = "notBreaching"

  dimensions = {
    LoadBalancer = aws_alb.this.arn_suffix
    TargetGroup  = aws_alb_target_group.website.arn_suffix
  }

  alarm_actions = local.alarm_sns_topics
  ok_actions    = local.alarm_sns_topics

  tags = merge(
    local.common_tags,
    {
      Name           = "${var.alb_name_prefix}-unhealthy-hosts"
      VantaOwner     = var.vanta_owner
      VantaCompliance = "Load balancer unhealthy host count monitored (AWS)"
    }
  )
}
```

#### Alarm 2: Target Response Time (Latency)

```hcl
resource "aws_cloudwatch_metric_alarm" "target_response_time" {
  count = local.alarms_enabled ? 1 : 0

  alarm_name          = "${var.alb_name_prefix}-high-latency"
  alarm_description   = "Triggers when target response time exceeds ${local.alarm_target_response_time}s (Vanta compliance)"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = var.alarm_evaluation_periods
  metric_name         = "TargetResponseTime"
  namespace           = "AWS/ApplicationELB"
  period              = 60  # 1 minute
  statistic           = "Average"
  threshold           = local.alarm_target_response_time
  treat_missing_data  = "notBreaching"

  dimensions = {
    LoadBalancer = aws_alb.this.arn_suffix
  }

  alarm_actions = local.alarm_sns_topics
  ok_actions    = local.alarm_sns_topics

  tags = merge(
    local.common_tags,
    {
      Name           = "${var.alb_name_prefix}-high-latency"
      VantaOwner     = var.vanta_owner
      VantaCompliance = "Load balancer latency monitored"
    }
  )
}
```

#### Alarm 3: Low Success Rate (Server Errors)

```hcl
resource "aws_cloudwatch_metric_alarm" "low_success_rate" {
  count = local.alarms_enabled ? 1 : 0

  alarm_name          = "${var.alb_name_prefix}-low-success-rate"
  alarm_description   = "Triggers when success rate drops below ${var.alarm_success_rate_threshold}% (Vanta compliance)"
  comparison_operator = "LessThanThreshold"
  evaluation_periods  = var.alarm_evaluation_periods
  threshold           = var.alarm_success_rate_threshold
  treat_missing_data  = "notBreaching"

  # Calculate success rate using metric math
  # Success rate = (2XX + 3XX + 4XX) / (2XX + 3XX + 4XX + 5XX) * 100
  metric_query {
    id          = "success_rate"
    expression  = "100 * (m2 + m3 + m4) / (m2 + m3 + m4 + m5)"
    label       = "Success Rate (%)"
    return_data = true
  }

  # 2xx responses (success)
  metric_query {
    id = "m2"
    metric {
      metric_name = "HTTPCode_Target_2XX_Count"
      namespace   = "AWS/ApplicationELB"
      period      = var.alarm_success_rate_period
      stat        = "Sum"
      dimensions = {
        LoadBalancer = aws_alb.this.arn_suffix
      }
    }
  }

  # 3xx responses (redirects - considered success)
  metric_query {
    id = "m3"
    metric {
      metric_name = "HTTPCode_Target_3XX_Count"
      namespace   = "AWS/ApplicationELB"
      period      = var.alarm_success_rate_period
      stat        = "Sum"
      dimensions = {
        LoadBalancer = aws_alb.this.arn_suffix
      }
    }
  }

  # 4xx responses (client errors - considered success from server perspective)
  metric_query {
    id = "m4"
    metric {
      metric_name = "HTTPCode_Target_4XX_Count"
      namespace   = "AWS/ApplicationELB"
      period      = var.alarm_success_rate_period
      stat        = "Sum"
      dimensions = {
        LoadBalancer = aws_alb.this.arn_suffix
      }
    }
  }

  # 5xx responses (server errors - failures)
  metric_query {
    id = "m5"
    metric {
      metric_name = "HTTPCode_Target_5XX_Count"
      namespace   = "AWS/ApplicationELB"
      period      = var.alarm_success_rate_period
      stat        = "Sum"
      dimensions = {
        LoadBalancer = aws_alb.this.arn_suffix
      }
    }
  }

  alarm_actions = local.alarm_sns_topics
  ok_actions    = local.alarm_sns_topics

  tags = merge(
    local.common_tags,
    {
      Name            = "${var.alb_name_prefix}-low-success-rate"
      VantaOwner      = var.vanta_owner
      VantaCompliance = "Load balancer server errors monitored (AWS)"
    }
  )
}
```

#### Alarm 4: High CPU Utilization

```hcl
resource "aws_cloudwatch_metric_alarm" "cpu_utilization" {
  count = local.alarms_enabled ? 1 : 0

  alarm_name          = "${var.alb_name_prefix}-high-cpu"
  alarm_description   = "Triggers when ASG CPU exceeds ${local.alarm_cpu_threshold}% for ${var.alarm_evaluation_periods * 5} minutes, indicating autoscaling failure (Vanta compliance)"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = var.alarm_evaluation_periods
  metric_name         = "CPUUtilization"
  namespace           = "AWS/EC2"
  period              = 300  # 5 minutes
  statistic           = "Average"
  threshold           = local.alarm_cpu_threshold
  treat_missing_data  = "notBreaching"

  dimensions = {
    AutoScalingGroupName = aws_autoscaling_group.website.name
  }

  alarm_actions = local.alarm_sns_topics
  ok_actions    = local.alarm_sns_topics

  tags = merge(
    local.common_tags,
    {
      Name            = "${var.alb_name_prefix}-high-cpu"
      VantaOwner      = var.vanta_owner
      VantaCompliance = "Server CPU monitored (AWS)"
    }
  )
}
```

**Alarm Design Rationale:**
- All alarms only created when `local.alarms_enabled = true`
- Use `treat_missing_data = "notBreaching"` to avoid false alarms during low traffic
- Send both alarm and OK notifications
- Tag with Vanta compliance mappings for audit trail
- Use appropriate statistics: Average for latency/CPU, Sum for error counts
- CPU alarm monitors AutoScaling Group aggregate, not individual instances
- **Period tuning by alarm type:**
  - Unhealthy hosts, latency: 60s (1 min) - immediate issues need fast detection
  - CPU: 300s (5 min) - allows autoscaling time to respond (~5-10 min total with evaluation_periods=2)
  - Success rate: Configurable (60s-3600s) - depends on traffic volume

---

### 5. Warning Check Block

**File:** `deprecations.tf`

```hcl
check "vanta_alarms_recommended" {
  assert {
    condition     = length(var.alarm_emails) > 0 || length(var.alarm_topic_arns) > 0
    error_message = <<-EOF
      ⚠️  VANTA COMPLIANCE RECOMMENDATION ⚠️

      No CloudWatch alarm notifications are configured for this ALB.

      **Vanta Compliance Requirements:**
      Vanta requires monitoring for:
      - Load balancer unhealthy host count
      - Load balancer latency
      - Load balancer server errors (5xx)
      - Server CPU utilization

      **To enable alarms, configure one of:**

      1. Email notifications:
         ```
         alarm_emails = ["ops-team@example.com"]
         ```

      2. Existing SNS topics:
         ```
         alarm_topic_arns = ["arn:aws:sns:us-east-1:123456789012:alerts"]
         ```

      ⚠️  **FUTURE REQUIREMENT:** In v6.0.0, alarm_emails will require at least one email address.

      **This is a warning only** - your deployment will proceed, but Vanta compliance checks may fail.
      See: https://www.vanta.com/products/trust-center
    EOF
  }
}
```

**Rationale:**
- Clear, actionable warning message
- Shows exact code users need to add
- Mentions v6.0.0 requirement
- Links to Vanta documentation
- Does not block deployment (warning only)

---

### 6. Validation Check Block

**File:** `deprecations.tf`

```hcl
check "cpu_alarm_threshold_sane" {
  assert {
    condition = local.alarm_cpu_threshold > var.autoscaling_target_cpu_load
    error_message = <<-EOF
      ⚠️  CPU ALARM CONFIGURATION ERROR ⚠️

      CPU alarm threshold (${local.alarm_cpu_threshold}%) must be greater than
      autoscaling target (${var.autoscaling_target_cpu_load}%).

      **Current configuration:**
      - autoscaling_target_cpu_load:     ${var.autoscaling_target_cpu_load}%
      - alarm_cpu_utilization_threshold: ${coalesce(var.alarm_cpu_utilization_threshold, "auto (${local.alarm_cpu_threshold}%)")}

      **The alarm should trigger AFTER autoscaling attempts to scale up.**
      If alarm threshold ≤ autoscaling target, the alarm will fire immediately
      without giving autoscaling a chance to respond.

      **To fix:**
      ```
      # Let it auto-calculate (recommended)
      # alarm_cpu_utilization_threshold defaults to autoscaling_target_cpu_load + 30%

      # OR manually set it higher:
      alarm_cpu_utilization_threshold = ${var.autoscaling_target_cpu_load + 30}
      ```
    EOF
  }
}
```

**Rationale:**
- Catches misconfiguration where alarm would trigger before autoscaling can respond
- Provides clear error message with current values
- Shows both auto-calculated and manual override options
- Prevents users from shooting themselves in the foot

---

### 7. New Outputs

**File:** `outputs.tf`

```hcl
output "alarm_sns_topic_arn" {
  description = "ARN of the SNS topic for ALB CloudWatch alarms (if created)"
  value       = length(aws_sns_topic.alarms) > 0 ? aws_sns_topic.alarms[0].arn : null
}

output "alarm_sns_topic_name" {
  description = "Name of the SNS topic for ALB CloudWatch alarms (if created)"
  value       = length(aws_sns_topic.alarms) > 0 ? aws_sns_topic.alarms[0].name : null
}

output "cloudwatch_alarm_arns" {
  description = "ARNs of CloudWatch alarms created for ALB and ASG monitoring"
  value = {
    unhealthy_hosts   = length(aws_cloudwatch_metric_alarm.unhealthy_host_count) > 0 ? aws_cloudwatch_metric_alarm.unhealthy_host_count[0].arn : null
    high_latency      = length(aws_cloudwatch_metric_alarm.target_response_time) > 0 ? aws_cloudwatch_metric_alarm.target_response_time[0].arn : null
    low_success_rate  = length(aws_cloudwatch_metric_alarm.low_success_rate) > 0 ? aws_cloudwatch_metric_alarm.low_success_rate[0].arn : null
    high_cpu          = length(aws_cloudwatch_metric_alarm.cpu_utilization) > 0 ? aws_cloudwatch_metric_alarm.cpu_utilization[0].arn : null
  }
}
```

**Rationale:**
- Exposes SNS topic ARN so users can add additional subscriptions
- Provides alarm ARNs for integration with other tools
- All outputs return null when alarms not enabled

---

## Files to Create/Modify

### New Files
1. **`alarms.tf`** - All alarm-related resources
2. **`.claude/plans/v5.12.0-vanta-alarms.md`** - This document

### Modified Files
1. **`variables.tf`** - Add new alarm variables at end of file
2. **`locals.tf`** - Add `alarms_enabled` and `alarm_sns_topics` locals
3. **`outputs.tf`** - Add alarm-related outputs
4. **`deprecations.tf`** - Add warning check block
5. **`cloudwatch.tf`** - Remove legacy CPU alarm (migrated to alarms.tf)
6. **`CHANGELOG.md`** - Document new feature
7. **`README.md`** - Will be auto-updated by terraform-docs

---

## Testing Strategy

### Automated Tests

**Integration Test: `test_create_lb` (EXTEND EXISTING)**

Extend the existing ALB test to verify alarm functionality:

```python
def test_create_lb(service_network, keep_after, aws_region, test_role_arn, subzone):
    """Test creating ALB with Vanta alarms enabled"""

    # ... existing test setup ...

    # ADD: Configure with test email (won't be confirmed, but that's OK)
    alarm_emails = ["terraform-test+alarms-noconfirm@infrahouse.com"]

    outputs = apply_terraform(terraform_dir, {
        # ... existing vars ...
        'alarm_emails': alarm_emails,
    })

    cw_client = boto3.client('cloudwatch', region_name=aws_region)
    sns_client = boto3.client('sns', region_name=aws_region)

    # 1. Verify SNS topic created
    topic_arn = outputs['alarm_sns_topic_arn']['value']
    assert topic_arn is not None, "SNS topic should be created"

    # 2. Verify email subscription exists (PendingConfirmation is acceptable)
    subs = sns_client.list_subscriptions_by_topic(TopicArn=topic_arn)
    email_sub = [s for s in subs['Subscriptions'] if s['Protocol'] == 'email'][0]
    assert email_sub['Endpoint'] == alarm_emails[0]

    # 3. Verify all 4 alarms exist
    alarm_arns = outputs['cloudwatch_alarm_arns']['value']
    assert alarm_arns['unhealthy_hosts'] is not None
    assert alarm_arns['high_latency'] is not None
    assert alarm_arns['low_success_rate'] is not None
    assert alarm_arns['high_cpu'] is not None

    # 4. Get alarm details
    alarm_names = [arn.split(':')[-1] for arn in alarm_arns.values()]
    alarms_response = cw_client.describe_alarms(AlarmNames=alarm_names)
    alarms = {a['AlarmName']: a for a in alarms_response['MetricAlarms']}

    # 5. Verify CPU alarm configuration
    cpu_alarm = [a for a in alarms.values() if 'high-cpu' in a['AlarmName']][0]
    assert cpu_alarm['Threshold'] == 90, "CPU threshold should be 90% (60% + 30%)"
    assert cpu_alarm['Period'] == 300, "CPU alarm period should be 5 minutes"
    assert cpu_alarm['EvaluationPeriods'] == 2
    assert cpu_alarm['Dimensions'][0]['Name'] == 'AutoScalingGroupName'
    assert topic_arn in cpu_alarm['AlarmActions']

    # 6. Verify unhealthy host alarm
    unhealthy_alarm = [a for a in alarms.values() if 'unhealthy-hosts' in a['AlarmName']][0]
    assert unhealthy_alarm['Threshold'] == 1
    assert unhealthy_alarm['Period'] == 60
    assert unhealthy_alarm['EvaluationPeriods'] == 2

    # 7. Verify latency alarm
    latency_alarm = [a for a in alarms.values() if 'high-latency' in a['AlarmName']][0]
    assert latency_alarm['Threshold'] == 48, "Should be 80% of default 60s idle timeout"
    assert latency_alarm['Period'] == 60

    # 8. Verify success rate alarm (uses metric math)
    success_alarm = [a for a in alarms.values() if 'low-success-rate' in a['AlarmName']][0]
    assert success_alarm['Threshold'] == 99.0
    assert 'Metrics' in success_alarm, "Success rate alarm should use metric math"

    # ... rest of existing test ...
```

**Rationale:**
- Reuses existing test infrastructure (no additional AWS resources)
- Fast execution (~30 seconds overhead)
- Email subscription won't be confirmed, but infrastructure validation is sufficient
- No need to actually trigger alarms - we're testing configuration, not CloudWatch service
- Validates correct alarm configuration and integration with ALB/ASG

### Manual Testing Checklist (Pre-Release)

- [ ] Deploy with `alarm_emails = ["your-real-email@example.com"]`
- [ ] Verify SNS confirmation email received
- [ ] Confirm SNS subscription
- [ ] Verify all 4 alarms appear in CloudWatch console (unhealthy hosts, latency, success rate, CPU)
- [ ] Verify alarms have correct thresholds
- [ ] Trigger unhealthy host alarm (stop target instance for 2+ minutes)
- [ ] Verify email notification received
- [ ] Verify alarm clears when instance becomes healthy
- [ ] Trigger CPU alarm: `stress --cpu 4 --timeout 600s` (10 minutes)
- [ ] Verify email notification received for CPU alarm
- [ ] Verify alarm clears when CPU drops
- [ ] Deploy without `alarm_emails` and verify warning check appears
- [ ] Verify terraform-docs updates README correctly
- [ ] Verify legacy CPU alarm in cloudwatch.tf is removed

---

## Documentation Updates

### CHANGELOG.md

```markdown
## [5.12.0] - 2025-MM-DD

### Added
- **Vanta Compliance:** Added optional CloudWatch alarms for ALB and ASG monitoring (#XXX)
  - Unhealthy host count alarm
  - Target response time (latency) alarm
  - Success rate alarm (monitors 5xx errors intelligently)
  - CPU utilization alarm
- New variable: `alarm_emails` - List of email addresses for alarm notifications
- New variable: `alarm_topic_arns` - List of SNS topic ARNs for alarm integrations
- New variable: `alarm_unhealthy_host_threshold` - Threshold for unhealthy hosts (default: 1)
- New variable: `alarm_target_response_time_threshold` - Latency threshold in seconds (defaults to 80% of alb_idle_timeout)
- New variable: `alarm_success_rate_threshold` - Minimum success rate percentage (default: 99.0%)
- New variable: `alarm_success_rate_period` - Period for success rate calculation (default: 300s, supports up to 3600s for low-traffic sites)
- New variable: `alarm_cpu_utilization_threshold` - CPU threshold percentage (auto-calculated from autoscaling target + 30%)
- New variable: `alarm_evaluation_periods` - Evaluation periods for alarms (default: 2)
- New output: `alarm_sns_topic_arn` - ARN of created SNS topic for alarms
- New output: `alarm_sns_topic_name` - Name of created SNS topic for alarms
- New output: `cloudwatch_alarm_arns` - Map of CloudWatch alarm ARNs
- Warning check block when alarms are not configured (prepares for v6.0.0 requirement)

### Changed
- Response time alarm threshold now intelligently defaults to 80% of `alb_idle_timeout`
- Success rate alarm uses metric math to calculate error rate, scaling with traffic volume
- Migrated legacy CPU alarm from `cloudwatch.tf` to unified alarm system in `alarms.tf`

### Deprecated
- Legacy `sns_topic_alarm_arn` variable (still supported but recommend using `alarm_emails` or `alarm_topic_arns`)

### Notes
- All alarm features are **optional** in v5.12.0
- In v6.0.0, `alarm_emails` will require at least one email for Vanta compliance
- See UPGRADE-6.0.md for migration details
```

### README.md

Will be auto-updated by terraform-docs with new variables and outputs.

Consider adding a "Vanta Compliance" section:

```markdown
## Vanta Compliance

This module supports Vanta compliance requirements for ALB monitoring:

### CloudWatch Alarms

Configure email notifications to receive alerts:

```hcl
module "website" {
  source  = "infrahouse/website-pod/aws"
  version = "~> 5.12"

  # Enable Vanta-required alarms
  alarm_emails = [
    "ops-team@example.com",
    "on-call@example.com"
  ]

  # Optional: Customize alarm thresholds
  alarm_unhealthy_host_threshold        = 1
  alarm_target_response_time_threshold  = 2.0
  alarm_5xx_threshold                   = 10

  # ... other configuration
}
```

The module will automatically create CloudWatch alarms for:
- Load balancer unhealthy host count
- Load balancer latency
- Load balancer server errors (5xx)
- Server CPU utilization

**Note:** In v6.0.0, at least one `alarm_emails` address will be required.
```

---

## Migration Impact

### For Existing Users (Upgrading to v5.12.0)

**No breaking changes:**
- All new variables have defaults
- No alarms created unless `alarm_emails` or `alarm_topic_arns` configured
- Warning appears but does not block deployment

**Recommended action:**
Configure `alarm_emails` to prepare for v6.0.0 and enable Vanta compliance.

### For New Users

Will see warning check block recommending alarm configuration.

---

## Success Criteria

v5.12.0 is ready for release when:

- ✅ All new variables added with proper descriptions
- ✅ SNS topic and subscriptions created conditionally
- ✅ All 4 CloudWatch alarms created with correct metrics (unhealthy hosts, latency, success rate, CPU)
- ✅ Legacy CPU alarm removed from cloudwatch.tf
- ✅ Warning check block provides clear guidance
- ✅ All tests passing
- ✅ Manual testing completed
- ✅ terraform-docs generates correct README
- ✅ CHANGELOG.md updated
- ✅ No breaking changes introduced

---

## Implementation Checklist

- [x] Create `alarms.tf` with SNS and CloudWatch resources
- [x] Add variables to `variables.tf`
- [x] Add locals to `locals.tf`
- [x] Add outputs to `outputs.tf`
- [x] Add warning check and validation check to `deprecations.tf`
- [x] Extend `tests/test_create_lb.py` with alarm verification
- [ ] Update CHANGELOG.md
- [ ] Run terraform-docs to update README
- [ ] Manual testing (with real email, trigger alarms)
- [ ] Code review
- [ ] Merge to main
- [ ] Release v5.12.0

---

**Document Created:** 2025-11-29
**Created By:** Claude Code
**Next Review:** After implementation
